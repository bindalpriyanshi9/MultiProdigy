import asyncio

# âœ… Import clients correctly
from MultiProdigy.llm.clients import OpenAIClient, GeminiClient, OllamaClient
from MultiProdigy.llm.huggingface_client import HuggingFaceClient

# âœ… HuggingFace (offline model)
async def demo_huggingface():
    try:
        client = HuggingFaceClient(model_name="distilgpt2")  # or "gpt2"
        result = await client.generate("Tell me about Artificial Intelligence.")
        print(f"\nğŸŸ¡ HuggingFace (Offline) Response: {result}")
    except Exception as e:
        print(f"âŒ HuggingFace Error: {e}")

# âœ… Simulated OpenAI
async def demo_openai():
    client = OpenAIClient()
    result = await client.generate("Hello from OpenAI")
    print(f"\nğŸ”· OpenAI Response: {result}")

# âœ… Simulated Gemini
async def demo_gemini():
    client = GeminiClient()
    result = await client.generate("Hello from Gemini")
    print(f"\nğŸŸ£ Gemini Response: {result}")

# âœ… Real Ollama
async def demo_ollama():
    client = OllamaClient(model="tinyllama")  # or any local model like llama3
    result = await client.generate("Hello from Ollama")
    print(f"\nğŸŸ¢ Ollama Response: {result}")

# âœ… Main orchestrator
async def main():
    print("ğŸŒ Running all LLM Clients Demo...\n")
    await demo_openai()
    await demo_gemini()
    await demo_huggingface()
    await demo_ollama()
    print("\nâœ… All clients ran successfully!")

# âœ… Run the main function
if __name__ == "__main__":
    asyncio.run(main())
